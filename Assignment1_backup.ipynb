{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590: Advanced Machine Learning\n",
    "# Assignment 1: The Super Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Image\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn import ensemble\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "\n",
    "\n",
    "# Add more packages as required\n",
    "\n",
    "%matplotlib inline\n",
    "#%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Super Learner Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Super Learner* is a heterogeneous stacked ensemble classifier. This is a classification model that uses a set of base classifiers of different types, the outputs of which are then combined in another classifier at the stacked layer. The Super Learner was described in [(van der Laan et al, 2007)](https://pdfs.semanticscholar.org/19e9/c732082706f39d2ba12845851309714db135.pdf) but the stacked ensemble idea has been around for a long time. \n",
    "\n",
    "Figure 1 shows a flow diagram of the Super Learner process (this is from (van der Laan et al, 2007) and the process is also described in the COMP47590 lecture \"[COMP47590 2017-2018 L04 Supervised Learning Ensembles 3](https://www.dropbox.com/s/1ksx94nxtuyn4l8/COMP47590%202017-2018%20L04%20Supervised%20Learning%20Ensembles%203.pdf?raw=1)\"). The base classifiers are trained and their outputs are combined along with the training dataset labels into a training set for the stack layer classifier. To avoid overfitting the generation of the stacked layer training set uses a k-fold cross validation process (described as V-fold in Figure 1). To further add variety to the base estimators a bootstrapping selection (as is used in the bagging ensemble approach).\n",
    " \n",
    "![Super Learner Process Flow](SuperLearnerProcessFlow.png \"Logo Title Text 1\")\n",
    "Figure 1: A flow diagram for the Super Learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the SuperLearnerClassifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_label = []\n",
    "test_index = []\n",
    "k_store =[]\n",
    "train_index = []\n",
    "class SuperLearnerClassifier(BaseEstimator, ClassifierMixin):\n",
    "       \n",
    "    def __init__(self, training_type, layer_type, estimators):\n",
    "        self.dt_model = None\n",
    "        self.rf_model = None\n",
    "        self.svm_model = None\n",
    "        self.lr_model = None\n",
    "        self.knn_model = None\n",
    "        self.nb_model = None\n",
    "        self.base_models = None\n",
    "        self.superlearner = None\n",
    "        self.layer_model =None\n",
    "        self.training_type  = training_type\n",
    "        self.layer_type = layer_type\n",
    "        self.estimators = estimators\n",
    "           \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        self.dt_model = DecisionTreeClassifier()\n",
    "        self.rf_model = RandomForestClassifier()\n",
    "        self.svm_model = SVC(probability=True)\n",
    "        self.lr_model = LogisticRegression()\n",
    "        self.knn_model = KNeighborsClassifier()\n",
    "        self.nb_model= GaussianNB()\n",
    "        \n",
    "        estimator_list = []\n",
    "        \n",
    "        for k,(train, test) in enumerate(kfold.split(X, y)):\n",
    "            \n",
    "            true_label.append(y[test])\n",
    "            train_index.append(train)\n",
    "            test_index.append(test)\n",
    "            k_store.append(k)\n",
    "            \n",
    "            if 'SVM' in self.estimators:\n",
    "                svm_fit = self.svm_model.fit(X.iloc[train], y[train])\n",
    "                print(\"[SVM fold {0}], fit SVM model\".format(k))\n",
    "                estimator_list.append(('svm', svm_fit))\n",
    "                \n",
    "            if 'Decision Tree' in self.estimators:\n",
    "                dt_fit = self.dt_model.fit(X.iloc[train], y[train])            \n",
    "                print(\"[Decision Tree fold {0}], fit decision tree model\".format(k))\n",
    "                estimator_list.append(('dt', dt_fit))\n",
    "            \n",
    "            if 'Ramdon Forest' in self.estimators:\n",
    "                rf_fit = self.rf_model.fit(X.iloc[train], y[train])\n",
    "                print(\"[Random Forest fold {0}], fit random forest model\".format(k))\n",
    "                estimator_list.append(('rf', rf_fit))\n",
    "            \n",
    "            if 'Logistical Regression' in self.estimators:\n",
    "                lr_fit = self.lr_model.fit(X.iloc[train], y[train])\n",
    "                print(\"[Logistical Regression fold {0}], fit logistical regression model\".format(k))\n",
    "                estimator_list.append(('lr', lr_fit))\n",
    "            \n",
    "            if 'KNN' in self.estimators:\n",
    "                knn_fit = self.knn_model.fit(X.iloc[train], y[train])\n",
    "                print(\"[KNN fold {0}], fit KNN model\".format(k))\n",
    "                estimator_list.append(('knn', knn_fit))\n",
    "            \n",
    "            if 'Navie Bayes' in self.estimators:\n",
    "                nb_fit = self.nb_model.fit(X.iloc[train], y[train])\n",
    "                print(\"[Navie Bayes fold {0}], fit navie bayes model\".format(k))\n",
    "                estimator_list.append(('nb', nb_fit))\n",
    "                \n",
    "            print(\"**************************\")\n",
    "        \n",
    "        print(estimator_list)\n",
    "        self.base_models = VotingClassifier(estimators = estimator_list, voting ='hard')\n",
    "            \n",
    "#             self.base_models = VotingClassifier(estimators = [('dt',self.dt_model),('rf',self.rf_model),\n",
    "#                                                          ('svm',self.svm_model), ('lr',self.lr_model),\n",
    "#                                                          ('knn',self.knn_model), ('nb',self.nb_model)], voting ='hard')\n",
    "            \n",
    "        label = np.concatenate(true_label[0:len(k_store)], axis =0)\n",
    "        if self.layer_type == 'Decision Tree':\n",
    "            self.layer_model = DecisionTreeClassifier() \n",
    "            \n",
    "            if self.training_type == 'label_base': \n",
    "                self.layer_model.fit(self.predict(X), label)\n",
    "                print('using label base traning data to fit stack layer classifier')\n",
    "            elif self.training_type == 'proba_base': \n",
    "                self.layer_model.fit(self.predict_proba(X), label)\n",
    "                print('using probability base traning data to fit stack layer classifier')\n",
    "                \n",
    "            print('stack layer model is Decision Tree ')    \n",
    "            self.superlearner = VotingClassifier(estimators = [('bm', self.base_models), \n",
    "                                                               ('dt', self.layer_model)],voting = 'hard')\n",
    "                                 \n",
    "        elif self.layer_type == 'Random Forest':\n",
    "            self.layer_model = RandomForestClassifier()\n",
    "            \n",
    "            if self.training_type == 'label_base': \n",
    "                self.layer_model.fit(self.predict(X), label)\n",
    "                print('using label base traning data to fit stack layer classifier')\n",
    "            elif self.training_type == 'proba_base': \n",
    "                self.layer_model.fit(self.predict_proba(X), label)\n",
    "                print('using probability base traning data to fit stack layer classifier')\n",
    "                \n",
    "            self.superlearner = VotingClassifier(estimators = [('bm',self.base_models), \n",
    "                                                               ('rf', self.layer_model)],voting = 'hard')\n",
    "            print('stack layer model is Random Forest ')\n",
    "            \n",
    "        elif self.layer_type == 'SVM':\n",
    "            self.layer_model = SVC(probability=True) \n",
    "            \n",
    "            if self.training_type == 'label_base': \n",
    "                self.layer_model.fit(self.predict(X), label)\n",
    "                print('using label base traning data to fit stack layer classifier')\n",
    "            elif self.training_type == 'proba_base': \n",
    "                self.layer_model.fit(self.predict_proba(X), label)\n",
    "                print('using probability base traning data to fit stack layer classifier')\n",
    "                \n",
    "            self.superlearner = VotingClassifier(estimators = [('bm',self.base_models), \n",
    "                                                               ('svm', self.layer_model)],voting = 'hard')\n",
    "            print('stack layer model is SVM')\n",
    "            \n",
    "            \n",
    "        elif self.layer_type == 'Logistic Regression':\n",
    "            self.layer_model = LogisticRegression()\n",
    "            \n",
    "            if self.training_type == 'label_base': \n",
    "                self.layer_model.fit(self.predict(X), label)\n",
    "                print('using label base traning data to fit stack layer classifier')\n",
    "            elif self.training_type == 'proba_base': \n",
    "                self.layer_model.fit(self.predict_proba(X), label)\n",
    "                print('using probability base traning data to fit stack layer classifier')\n",
    "                \n",
    "            self.superlearner = VotingClassifier(estimators = [('bm',self.base_models), \n",
    "                                                               ('lr', self.layer_model)],voting = 'hard')\n",
    "            print('stack layer model is Logistic Regression ')\n",
    "            \n",
    "        elif self.layer_type == 'KNN':\n",
    "            self.layer_model = KNeighborsClassifier()\n",
    "\n",
    "            if self.training_type == 'label_base': \n",
    "                self.layer_model.fit(self.predict(X), label)\n",
    "                print('using label base traning data to fit stack layer classifier')\n",
    "            elif self.training_type == 'proba_base': \n",
    "                self.layer_model.fit(self.predict_proba(X), label)\n",
    "                print('using probability base traning data to fit stack layer classifier')\n",
    "                \n",
    "            self.superlearner = VotingClassifier(estimators = [('bm', self.base_models), \n",
    "                                                               ('knn', self.layer_model)],voting = 'hard')        \n",
    "            print('stack layer model is KNN ')\n",
    "                 \n",
    "        print('**************************')\n",
    "        print('CONSTRUCT AND FIT A SUPERLEARNER')\n",
    "        return self.superlearner.fit(X,y)\n",
    "          \n",
    "#         return self.superlearner.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "                                        \n",
    "    def predict(self, X):\n",
    "        \n",
    "        base_svm_pred_list = []\n",
    "        base_rf_pred_list = []\n",
    "        base_dt_pred_list = []\n",
    "        base_lr_pred_list = []\n",
    "        base_knn_pred_list = []\n",
    "        base_nb_pred_list = []\n",
    "            \n",
    "        for i in test_index:\n",
    "            base_svm_pred = self.svm_model.predict(X.iloc[i])\n",
    "            base_svm_pred_list.append(base_svm_pred)\n",
    "\n",
    "            base_dt_pred = self.dt_model.predict(X.iloc[i])\n",
    "            base_dt_pred_list.append(base_dt_pred)\n",
    "\n",
    "            base_rf_pred = self.rf_model.predict(X.iloc[i])\n",
    "            base_rf_pred_list.append(base_rf_pred)\n",
    "\n",
    "            base_lr_pred = self.lr_model.predict(X.iloc[i])\n",
    "            base_lr_pred_list.append(base_lr_pred)\n",
    "\n",
    "            base_knn_pred = self.knn_model.predict(X.iloc[i])\n",
    "            base_knn_pred_list.append(base_knn_pred)\n",
    "\n",
    "            base_nb_pred = self.nb_model.predict(X.iloc[i])\n",
    "            base_nb_pred_list.append(base_nb_pred)\n",
    "\n",
    "        # construct traning data for stack layer classifier (label-based)\n",
    "        svm_data = np.concatenate(base_svm_pred_list[0:len(base_svm_pred_list)])\n",
    "        dt_data = np.concatenate(base_dt_pred_list[0:len(base_dt_pred_list)])\n",
    "        rf_data = np.concatenate(base_rf_pred_list[0:len(base_rf_pred_list)])\n",
    "        lr_data = np.concatenate(base_lr_pred_list[0:len(base_lr_pred_list)])\n",
    "        knn_data = np.concatenate(base_knn_pred_list[0:len(base_knn_pred_list)])\n",
    "        nb_data = np.concatenate(base_nb_pred_list[0:len(base_nb_pred_list)])\n",
    "\n",
    "        conbination_list = [svm_data, dt_data, rf_data, lr_data, knn_data, nb_data]\n",
    "        training_data_label_base = pd.DataFrame(conbination_list).T\n",
    "            \n",
    "        return training_data_label_base\n",
    "                \n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \n",
    "        base_svm_proba_list = []\n",
    "        base_rf_proba_list = []\n",
    "        base_dt_proba_list = []\n",
    "        base_lr_proba_list = []\n",
    "        base_knn_proba_list = []\n",
    "        base_nb_proba_list = []\n",
    "        \n",
    "        for j in test_index:\n",
    "            base_svm_proba = self.svm_model.predict_proba(X.iloc[j])\n",
    "            base_svm_proba_list.append(base_svm_proba)\n",
    "            \n",
    "            base_dt_proba = self.dt_model.predict_proba(X.iloc[j])\n",
    "            base_dt_proba_list.append(base_dt_proba)\n",
    "            \n",
    "            base_rf_proba = self.rf_model.predict_proba(X.iloc[j])\n",
    "            base_rf_proba_list.append(base_rf_proba)\n",
    "            \n",
    "            base_lr_proba = self.lr_model.predict_proba(X.iloc[j])\n",
    "            base_lr_proba_list.append(base_lr_proba)\n",
    "            \n",
    "            base_knn_proba = self.knn_model.predict_proba(X.iloc[j])\n",
    "            base_knn_proba_list.append(base_knn_proba)\n",
    "                        \n",
    "            base_nb_proba = self.nb_model.predict_proba(X.iloc[j])\n",
    "            base_nb_proba_list.append(base_nb_proba)\n",
    "        \n",
    "        # construct traning data for stack layer classifier (probability-based)\n",
    "        svm_proba_data = np.concatenate(base_svm_proba_list[0:len(base_svm_proba_list)])\n",
    "        dt_proba_data = np.concatenate(base_dt_proba_list[0:len(base_dt_proba_list)])\n",
    "        rf_proba_data = np.concatenate(base_rf_proba_list[0:len(base_rf_proba_list)])\n",
    "        lr_proba_data = np.concatenate(base_lr_proba_list[0:len(base_lr_proba_list)])\n",
    "        knn_proba_data = np.concatenate(base_knn_proba_list[0:len(base_knn_proba_list)])\n",
    "        nb_proba_data = np.concatenate(base_nb_proba_list[0:len(base_nb_proba_list)])\n",
    "        training_data_proba_base = np.concatenate([svm_proba_data, dt_proba_data, rf_proba_data, \n",
    "                                      lr_proba_data, knn_proba_data, nb_proba_data ], axis = 1)        \n",
    "        return training_data_proba_base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a simple test using the SuperLearnClassifier on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.datasets import load_iris\n",
    "# iris = load_iris()\n",
    "# clf = SuperLearnerClassifier()\n",
    "# cv_folds = 2\n",
    "# clf.fit(iris.data, iris.target)\n",
    "# clf.predict(iris.data)\n",
    "# clf.predict_proba(iris.data)\n",
    "# x = cross_val_score(clf, iris.data, iris.target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only a sample of the dataset for fast testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sampling_rate = 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the number of folds for all grid searches (should be 5 - 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv_folds = 5\n",
    "kfold = KFold(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "      <th>pixel784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2947</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>...</td>\n",
       "      <td>123</td>\n",
       "      <td>128</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17989</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>96</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6447</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37821</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>91</td>\n",
       "      <td>85</td>\n",
       "      <td>97</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2858</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "2947       2       0       0       0       0       0       0       0       0   \n",
       "17989      6       0       0       0       0       0       0       0       0   \n",
       "6447       7       0       0       0       0       0       0       0       0   \n",
       "37821      8       0       0       0       0       0       0       0       0   \n",
       "2858       7       0       0       0       0       0       0       0       0   \n",
       "\n",
       "       pixel9    ...     pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "2947       25    ...          123       128       123         0         0   \n",
       "17989       0    ...           96        10         0         0         0   \n",
       "6447        0    ...            0         0         0         0         0   \n",
       "37821       0    ...           93        93        93        91        85   \n",
       "2858        0    ...            0         0         0         0         0   \n",
       "\n",
       "       pixel780  pixel781  pixel782  pixel783  pixel784  \n",
       "2947          0         0         0         0         0  \n",
       "17989         0         0         0         0         0  \n",
       "6447          0         0         0         0         0  \n",
       "37821        97        22         0         0         0  \n",
       "2858          0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = pd.read_csv('fashion-mnist_train.csv')\n",
    "dataset = dataset.sample(frac=data_sampling_rate) #take a sample from the dataset so everyhting runs smoothly\n",
    "num_classes = 10\n",
    "classes = {0: \"T-shirt/top\", 1:\"Trouser\", 2: \"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle boot\"}\n",
    "display(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data pre-processing and manipulation as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X - train set y - test set \n",
    "X = dataset[dataset.columns[1:]]\n",
    "y = np.array(dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalise the data\n",
    "X = X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split the data into a training set, a vaidation set, and a test set\n",
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n",
    "    = train_test_split(X, y, random_state=0, \\\n",
    "                                    train_size = 0.7)\n",
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = train_test_split(X_train_plus_valid, \\\n",
    "                                        y_train_plus_valid, \\\n",
    "                                        random_state=0, \\\n",
    "                                        train_size = 0.5/0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate a Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Super Learner Classifier using the prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SVM fold 0], fit SVM model\n",
      "[Decision Tree fold 0], fit decision tree model\n",
      "[Random Forest fold 0], fit random forest model\n",
      "[Logistical Regression fold 0], fit logistical regression model\n",
      "[KNN fold 0], fit KNN model\n",
      "[Navie Bayes fold 0], fit navie bayes model\n",
      "**************************\n",
      "[SVM fold 1], fit SVM model\n",
      "[Decision Tree fold 1], fit decision tree model\n",
      "[Random Forest fold 1], fit random forest model\n",
      "[Logistical Regression fold 1], fit logistical regression model\n",
      "[KNN fold 1], fit KNN model\n",
      "[Navie Bayes fold 1], fit navie bayes model\n",
      "**************************\n",
      "[SVM fold 2], fit SVM model\n",
      "[Decision Tree fold 2], fit decision tree model\n",
      "[Random Forest fold 2], fit random forest model\n",
      "[Logistical Regression fold 2], fit logistical regression model\n",
      "[KNN fold 2], fit KNN model\n",
      "[Navie Bayes fold 2], fit navie bayes model\n",
      "**************************\n",
      "[SVM fold 3], fit SVM model\n",
      "[Decision Tree fold 3], fit decision tree model\n",
      "[Random Forest fold 3], fit random forest model\n",
      "[Logistical Regression fold 3], fit logistical regression model\n",
      "[KNN fold 3], fit KNN model\n",
      "[Navie Bayes fold 3], fit navie bayes model\n",
      "**************************\n",
      "[SVM fold 4], fit SVM model\n",
      "[Decision Tree fold 4], fit decision tree model\n",
      "[Random Forest fold 4], fit random forest model\n",
      "[Logistical Regression fold 4], fit logistical regression model\n",
      "[KNN fold 4], fit KNN model\n",
      "[Navie Bayes fold 4], fit navie bayes model\n",
      "**************************\n",
      "[('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')), ('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')), ('nb', GaussianNB(priors=None)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')), ('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')), ('nb', GaussianNB(priors=None)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')), ('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')), ('nb', GaussianNB(priors=None)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')), ('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')), ('nb', GaussianNB(priors=None)), ('svm', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=True, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False)), ('dt', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            presort=False, random_state=None, splitter='best')), ('rf', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)), ('lr', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)), ('knn', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')), ('nb', GaussianNB(priors=None))]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using label base traning data to fit stack layer classifier\n",
      "stack layer model is Decision Tree \n",
      "**************************\n",
      "CONSTRUCT AND FIT A SUPERLEARNER\n"
     ]
    }
   ],
   "source": [
    "#stack learner is decision tree\n",
    "clf1 = SuperLearnerClassifier('label_base', 'Decision Tree', ['SVM', 'Decision Tree','Ramdon Forest','Logistical Regression','KNN','Navie Bayes'])\n",
    "superlearner1 = clf1.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "# clf.predict(X_train_plus_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = superlearner1.predict(X_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the trained classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluate the superLearnerClassifier\n",
    "accuracy = metrics.accuracy_score(pred, y_valid)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Experiment (Task 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfrom a 10-fold cross validation experiment to evaluate the performance of the SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(superlearner1, X_test, y_test, cv=10)\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Performance of Different Stack Layer Approaches (Task 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of the ensemble when a label based stack layer training set and a probability based stack layer training set is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stack layer is decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = superlearner1.predict(X_test)\n",
    "accuracy1 = metrics.accuracy_score(pred1, y_test)\n",
    "accuracy1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = SuperLearnerClassifier('proba_base', 'Decision Tree', ['SVM', 'Decision Tree','Ramdon Forest','Logistical Regression','KNN','Navie Bayes'])\n",
    "superlearner2 = clf2.fit(X_train_plus_valid, y_train_plus_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred2 = superlearner2.predict(X_test)\n",
    "accuracy2 = metrics.accuracy_score(pred2, y_test)\n",
    "accuracy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack layer is logistic regression | input - label base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = SuperLearnerClassifier('label_base', 'Logistic Regression', ['SVM', 'Decision Tree','Ramdon Forest','Logistical Regression','KNN','Navie Bayes'])\n",
    "superlearner3 = clf3.fit(X_train_plus_valid, y_train_plus_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred3 = superlearner3.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(pred3, y_test)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Stack layer is logistic regression | input - probability base data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf4 = SuperLearnerClassifier('proba_base', 'Logistic Regression', ['SVM', 'Decision Tree','Ramdon Forest','Logistical Regression','KNN','Navie Bayes'])\n",
    "superlearner4 = clf4.fit(X_train_plus_valid, y_train_plus_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred4 = superlearner4.predict(X_test)\n",
    "accuracy4 = metrics.accuracy_score(pred4, y_test)\n",
    "accuracy4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Through SuperLearnerClassifier Architectures & Parameters (Task 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfrom a grid search experiment to detemrine the optimal architecture and hyper-parameter values for the SuperLearnClasssifier for the MNIST Fashion classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the performance of the model selected by the grid search on a hold-out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Impact of Adding Original Descriptive Features at the Stack Layer (Task 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the impact of adding original descriptive features at the stack layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Ensemble Model (Task 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an analysis to investigate the strength of the base estimators and the strengths of the correlations between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Add code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP47590: Advanced Machine Learning\n",
    "# Assignment 1: The Super Learner\n",
    "# SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports Use Packages Etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML, Image\n",
    "\n",
    "from TAS_Python_Utilities import data_viz\n",
    "from TAS_Python_Utilities import data_viz_target\n",
    "from TAS_Python_Utilities import visualize_tree\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pyplot\n",
    "from random import randint\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.tree import export_graphviz\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn import tree\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import ensemble\n",
    "from sklearn import linear_model\n",
    "from sklearn import neighbors\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "%matplotlib inline\n",
    "#%qtconsole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Super Learner Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *Super Learner* is a heterogeneous stacked ensemble classifier. This is a classification model that uses a set of base classifiers of different types, the outputs of which are then combined in another classifier at the stacked layer. The Super Learner was described in [(van der Laan et al, 2007)](https://pdfs.semanticscholar.org/19e9/c732082706f39d2ba12845851309714db135.pdf) but the stacked ensemble idea has been around for a long time. \n",
    "\n",
    "Figure 1 shows a flow diagram of the Super Learner process (this is from (van der Laan et al, 2007) and the process is also described in the COMP47590 lecture \"[COMP47590 2017-2018 L04 Supervised Learning Ensembles 3](https://www.dropbox.com/s/1ksx94nxtuyn4l8/COMP47590%202017-2018%20L04%20Supervised%20Learning%20Ensembles%203.pdf?raw=1)\"). The base classifiers are trained and their outputs are combined along with the training dataset labels into a training set for the stack layer classifier. To avoid overfitting the generation of the stacked layer training set uses a k-fold cross validation process (described as V-fold in Figure 1). To further add variety to the base estimators a bootstrapping selection (as is used in the bagging ensemble approach).\n",
    " \n",
    "![Super Learner Process Flow](SuperLearnerProcessFlow.png \"Logo Title Text 1\")\n",
    "Figure 1: A flow diagram for the Super Learner\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the SuperLearnerClassifier Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new classifier which is based on the sckit-learn BaseEstimator and ClassifierMixin classes\n",
    "class SuperLearnerClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \n",
    "    \"\"\"An ensemble classifier that uses heterogeneous models at the base layer and a aggregatnio model at the aggregation layer. A k-fold cross validation is used to gnerate training data for the stack layer model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    base_estimators: list \n",
    "        A list of the classifiers in the ase layer of the ensemble. Supported types are\n",
    "        - \"svm\" Support Vector Machine implemented by sklearn.svm.SVC\n",
    "        - \"logreg\" Logistic Regression implemented by sklearn.linear_models.LogisticRegression\n",
    "        - \"knn\" k Nearest Neighbour implemented by sklearn.neighbors.KNeighborsClassifier\n",
    "        - \"tree\" Decision Tree implemented by sklearn.tree.DecisionTreeClassifier\n",
    "        - \"randomforest\" RandomForest implemented by sklearn.tree.RandomForestClassifier    \n",
    "    classifier_duplicates: int, optional (default = 1)\n",
    "        How many instances of each classifier type listed in base_estimators is included in the ensemble\n",
    "    use_probs: boolean, optional (default = True)\n",
    "        Whether labels or probabilities are generated and used for the stack layer traiing data. \n",
    "    stack_layer_classifier: string, optional (default = \"logreg')\n",
    "        The classifier type used at the stack layer. The same classifier types as are supported at the base layer are supported        \n",
    "    training_folds: int, optional (default = 4)\n",
    "        How many folds will be used to generate the training set for the stacked layer\n",
    "    include_base_features_at_stack: boolean, optional (default = False)\n",
    "        Whether or not the base feature values should be includeds as inputs at the stack layer\n",
    "        \n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array of shape = [n_classes] \n",
    "        The classes labels (single output problem).\n",
    "\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The default values for most base learners are used.\n",
    "\n",
    "    See also\n",
    "    --------\n",
    "    \n",
    "    ----------\n",
    "    .. [1]  van der Laan, M., Polley, E. & Hubbard, A. (2007). \n",
    "            Super Learner. Statistical Applications in Genetics \n",
    "            and Molecular Biology, 6(1) \n",
    "            doi:10.2202/1544-6115.1309\n",
    "    Examples\n",
    "    --------\n",
    "    >>> from sklearn.datasets import load_iris\n",
    "    >>> from sklearn.model_selection import cross_val_score\n",
    "    >>> clf = SuperLearnerClassifier()\n",
    "    >>> iris = load_iris()\n",
    "    >>> cross_val_score(clf, iris.data, iris.target, cv=10)\n",
    "\n",
    "    \"\"\"\n",
    "    # Constructor for the classifier object\n",
    "    def __init__(self, base_estimators = [\"svm\", \"logreg\", \"knn\", \"tree\", \"randomforest\"], estimator_duplicates = 1, use_probs = False, stack_layer_classifier_type = \"tree\", training_folds = 4, include_base_features_at_stack = False):\n",
    "        \"\"\"Setup a SuperLearner classifier .\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "\n",
    "        \"\"\"     \n",
    "        self.base_estimators = base_estimators\n",
    "        self.base_estimator_types = list()\n",
    "        self.estimator_duplicates = estimator_duplicates\n",
    "        self.use_probs = use_probs\n",
    "        self.stack_layer_classifier_type = stack_layer_classifier_type\n",
    "        self.training_folds = training_folds\n",
    "        self.include_base_features_at_stack = include_base_features_at_stack\n",
    "\n",
    "    # The fit function to train a classifier\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build a SuperLearner classifier from the training set (X, y).\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape = [n_samples, n_features]\n",
    "            The training input samples. \n",
    "        y : array-like, shape = [n_samples] \n",
    "            The target values (class labels) as integers or strings.\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"    \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        # Store the classes seen during fit\n",
    "        self.classes_ = unique_labels(y)\n",
    "        \n",
    "        # Set up the base classifeirs in the ensemble\n",
    "        classifiers_ = list()\n",
    "        for i in range(0, self.estimator_duplicates):\n",
    "            for t in self.base_estimators:\n",
    "                self.base_estimator_types.append(t)\n",
    "                if t== \"svm\":\n",
    "                    c = svm.SVC(probability=True)\n",
    "\n",
    "                elif t == \"logreg\":\n",
    "                    c = linear_model.LogisticRegression()\n",
    "\n",
    "                elif t == \"knn\":\n",
    "                    c = neighbors.KNeighborsClassifier()\n",
    "\n",
    "                elif t == \"tree\":\n",
    "                    c = tree.DecisionTreeClassifier(min_samples_split=200)\n",
    "\n",
    "                elif t == \"randomforest\":\n",
    "                    c = ensemble.RandomForestClassifier()\n",
    "\n",
    "                classifiers_.append(c)\n",
    "        \n",
    "        self.n_estimators_ = len(classifiers_)\n",
    "        \n",
    "        # divide the dataset into k-folds\n",
    "        skf = StratifiedKFold(n_splits=self.training_folds)\n",
    "        skf.get_n_splits(X, y)\n",
    "\n",
    "        self.X_stack_train = None #(dtype = float)\n",
    "        self.y_stack_train = np.array([]) #(dtype = float)\n",
    "        # Iterate through the folds training models and using the test splits to generate training data for the stack layer\n",
    "        for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "            self.y_stack_train = np.r_[self.y_stack_train, y_test]\n",
    "            \n",
    "            if self.include_base_features_at_stack:\n",
    "                X_stack_train_fold = X_test\n",
    "            else:\n",
    "                X_stack_train_fold = None\n",
    "            \n",
    "            for classifier in classifiers_:\n",
    "                    \n",
    "                X_train_samp, y_train_samp = resample(X_train, y_train, replace=True)    \n",
    "                classifier.fit(X_train_samp, y_train_samp)\n",
    "                if not self.use_probs:\n",
    "                    y_pred = classifier.predict(X_test)\n",
    "                else:\n",
    "                    y_pred = classifier.predict_proba(X_test)\n",
    "                \n",
    "                try:\n",
    "                    X_stack_train_fold = np.c_[X_stack_train_fold, y_pred]\n",
    "                except ValueError:\n",
    "                    X_stack_train_fold = y_pred\n",
    "    \n",
    "            try:\n",
    "                self.X_stack_train = np.r_[self.X_stack_train, X_stack_train_fold]\n",
    "            except ValueError:\n",
    "                self.X_stack_train = X_stack_train_fold\n",
    "        \n",
    "            # Train the stack layer using the newly created dataset\n",
    "            if self.stack_layer_classifier_type == \"svm\":\n",
    "                self.stack_layer_classifier_ = svm.SVC(probability=True)\n",
    "                \n",
    "            elif self.stack_layer_classifier_type == \"logreg\":\n",
    "                self.stack_layer_classifier_ = linear_model.LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=1000)\n",
    "                \n",
    "            elif self.stack_layer_classifier_type == \"knn\":\n",
    "                self.stack_layer_classifier_ = neighbors.KNeighborsClassifier()\n",
    "                \n",
    "            elif self.stack_layer_classifier_type == \"tree\":\n",
    "                self.stack_layer_classifier_ = tree.DecisionTreeClassifier(min_samples_split=200)\n",
    "\n",
    "            elif self.stack_layer_classifier_type == \"randomforest\":\n",
    "                self.stack_layer_classifier_ = ensemble.RandomForestClassifier()\n",
    "                \n",
    "            self.stack_layer_classifier_.fit(self.X_stack_train, self.y_stack_train)\n",
    "            \n",
    "            # Retrain the base classifiers in the ensemble using the full dataset\n",
    "            self.classifiers_ = list()\n",
    "            for i in range(0, self.estimator_duplicates):\n",
    "                for t in self.base_estimators:\n",
    "                    if t == \"svm\":\n",
    "                        c = svm.SVC(probability = True)\n",
    "\n",
    "                    elif t == \"logreg\":\n",
    "                        c = linear_model.LogisticRegression()\n",
    "\n",
    "                    elif t == \"knn\":\n",
    "                        c = neighbors.KNeighborsClassifier()\n",
    "\n",
    "                    elif t == \"tree\":\n",
    "                        c = tree.DecisionTreeClassifier(min_samples_split=200)\n",
    "\n",
    "                    elif t == \"randomforest\":\n",
    "                        c = ensemble.RandomForestClassifier()\n",
    "\n",
    "                    # Perform a bootstrap sample\n",
    "                    X_samp, y_samp = resample(X, y, replace=True)\n",
    "                    c.fit(X_samp, y_samp)\n",
    "                    self.classifiers_.append(c)\n",
    "            \n",
    "        # Return the classifier\n",
    "        return self\n",
    "\n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class labels of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, ].\n",
    "            The predicted class labels of the input samples. \n",
    "        \"\"\"\n",
    "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
    "        check_is_fitted(self, ['stack_layer_classifier_'])\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "        \n",
    "        X_stack_label_queries = None\n",
    "        \n",
    "        if self.include_base_features_at_stack:\n",
    "            X_stack_queries = X\n",
    "        else:\n",
    "            X_stack_queries = None\n",
    "                \n",
    "        for classifier in self.classifiers_:\n",
    "            \n",
    "            y_pred_labels = classifier.predict(X)\n",
    "            y_pred_probs = classifier.predict_proba(X)\n",
    "            \n",
    "            if not self.use_probs:\n",
    "                y_pred = y_pred_labels\n",
    "            else:\n",
    "                y_pred = y_pred_probs\n",
    "                \n",
    "            try:\n",
    "                X_stack_queries = np.c_[X_stack_queries, y_pred]\n",
    "            except ValueError:\n",
    "                X_stack_queries = y_pred\n",
    "                \n",
    "            try:\n",
    "                X_stack_label_queries = np.c_[X_stack_label_queries, y_pred_labels]\n",
    "            except ValueError:\n",
    "                X_stack_label_queries = y_pred_labels\n",
    "        \n",
    "        self.last_X_stack_queries = X_stack_label_queries\n",
    "        \n",
    "        return self.stack_layer_classifier_.predict(X_stack_queries)\n",
    "    \n",
    "    # The predict function to make a set of predictions for a set of query instances\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict class probabilities of the input samples X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like matrix of shape = [n_samples, n_features]\n",
    "            The input samples. \n",
    "        Returns\n",
    "        -------\n",
    "        p : array of shape = [n_samples, n_labels].\n",
    "            The predicted class label probabilities of the input samples. \n",
    "        \"\"\"\n",
    "        # Check is fit had been called by confirming that the teamplates_ dictiponary has been set up\n",
    "        check_is_fitted(self, ['stack_layer_classifier_'])\n",
    "\n",
    "        # Check that the input features match the type and shape of the training features\n",
    "        X = check_array(X)\n",
    "        \n",
    "        if self.include_base_features_at_stack:\n",
    "            X_stack_queries = X\n",
    "        else:\n",
    "            X_stack_queries = None\n",
    "        X_stack_label_queries = None\n",
    "        \n",
    "        for classifier in self.classifiers_:\n",
    "            \n",
    "            y_pred_labels = classifier.predict(X)\n",
    "            y_pred_probs = classifier.predict_proba(X)\n",
    "            \n",
    "            if not self.use_probs:\n",
    "                y_pred = y_pred_labels\n",
    "            else:\n",
    "                y_pred = y_pred_probs\n",
    "                \n",
    "            try:\n",
    "                X_stack_queries = np.c_[X_stack_queries, y_pred]\n",
    "            except ValueError:\n",
    "                X_stack_queries = y_pred\n",
    "                \n",
    "            try:\n",
    "                X_stack_label_queries = np.c_[X_stack_label_queries, y_pred_labels]\n",
    "            except ValueError:\n",
    "                X_stack_label_queries = y_pred_labels\n",
    "        \n",
    "        self.last_X_stack_queries = X_stack_label_queries\n",
    "        \n",
    "        return self.stack_layer_classifier_.predict_proba(X_stack_queries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a simple test using the SuperLearnClassifier on the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "clf = SuperLearnerClassifier()\n",
    "cross_val_score(clf, iris.data, iris.target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a grid search using Iris data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = [\n",
    " {'base_estimators': [[\"tree\", \"logreg\"], [\"svm\", \"logreg\", \"knn\", \"tree\"], [\"svm\", \"logreg\", \"knn\", \"tree\", \"randomforest\"]], \n",
    "  'stack_layer_classifier_type':[\"tree\", \"logreg\"],\n",
    "  'estimator_duplicates':[1,2,5,10], \n",
    "  'training_folds': list(range(4, 11, 3)), \n",
    "  'use_probs':[False, True], \n",
    "  'include_base_features_at_stack':[False, True]}  \n",
    "]\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model = GridSearchCV(SuperLearnerClassifier(), param_grid, cv=2, verbose = 1, n_jobs=-1)\n",
    "my_tuned_model.fit(iris.data, iris.target)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(my_tuned_model.best_params_)\n",
    "print(my_tuned_model.best_score_)\n",
    "display(my_tuned_model.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup - IMPORTANT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take only a sample of the dataset for fast testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sampling_rate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup the number of folds for all grid searches (should be 5 - 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_folds = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset and explore it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('fashion-mnist_train.csv')\n",
    "dataset = dataset.sample(frac=data_sampling_rate) #take a sample from the dataset so everyhting runs smoothly\n",
    "num_classes = 10\n",
    "classes = {0: \"T-shirt/top\", 1:\"Trouser\", 2: \"Pullover\", 3:\"Dress\", 4:\"Coat\", 5:\"Sandal\", 6:\"Shirt\", 7:\"Sneaker\", 8:\"Bag\", 9:\"Ankle boot\"}\n",
    "display(dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process & Partition Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform data pre-processing and manipulation as required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = dataset[dataset.columns[1:]]\n",
    "Y = np.array(dataset[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_plus_valid, X_test, y_train_plus_valid, y_test \\\n",
    "    = train_test_split(X, Y, random_state=0, \\\n",
    "                                    train_size = 0.7)\n",
    "\n",
    "X_train, X_valid, y_train, y_valid \\\n",
    "    = train_test_split(X_train_plus_valid, \\\n",
    "                                        y_train_plus_valid, \\\n",
    "                                        random_state=0, \\\n",
    "                                        train_size = 0.5/0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Evaluate a Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a Super Learner Classifier using the prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = SuperLearnerClassifier()\n",
    "my_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model.predict(X_train)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_train, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_train, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "display(pd.crosstab(np.array(y_train), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the perfromance of the individual models in the stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of predictions for the training data\n",
    "my_model.predict(X_train)\n",
    "\n",
    "for i in range(my_model.n_estimators_):\n",
    "    print(\"** \", i, \" \", my_model.base_estimator_types[i])\n",
    "    \n",
    "    # Isolate the predictions from each model\n",
    "    y_pred = my_model.last_X_stack_queries[:, i]\n",
    "\n",
    "    # Print performance details\n",
    "    accuracy = metrics.accuracy_score(y_train, y_pred) # , normalize=True, sample_weight=None\n",
    "    print(\"Accuracy: \" +  str(accuracy))\n",
    "    print(metrics.classification_report(y_train, y_pred))\n",
    "\n",
    "    # Print nicer homemade confusion matrix\n",
    "    print(\"Confusion Matrix\")\n",
    "    display(pd.crosstab(np.array(y_train), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of predictions for the training data\n",
    "y_pred = my_model.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print nicer homemade confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "my_model.predict(X_test)\n",
    "\n",
    "for i in range(my_model.n_estimators_):\n",
    "    print(\"** \", i, \" \", my_model.base_estimator_types[i])\n",
    "    \n",
    "    # Make a set of predictions for the training data\n",
    "    y_pred = my_model.last_X_stack_queries[:, i]\n",
    "\n",
    "    # Print performance details\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "    print(\"Accuracy: \" +  str(accuracy))\n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "    # Print nicer homemade confusion matrix\n",
    "    print(\"Confusion Matrix\")\n",
    "    display(pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do a Cross Validation Experiment With SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfrom a 10-fold cross validation experiment to evaluate the perofrmance of the SuperLearnerClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = SuperLearnerClassifier()\n",
    "scores = cross_val_score(my_model, X_train_plus_valid, y_train_plus_valid, cv=cv_folds, n_jobs=-1, verbose = 2)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the Performance of Different Stack Layer Approaches (Task 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the performance of the ensemble when a label based stack layer training set and a probability based stack layer training set is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = SuperLearnerClassifier(use_probs=True, stack_layer_classifier_type=\"logreg\")\n",
    "scores = cross_val_score(my_model, X_train_plus_valid, y_train_plus_valid, cv=cv_folds, n_jobs=-1, verbose = 2)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search Through SuperLearnerClassifier Architectures & Parameters (Task 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfrom a grid search experiment to detemrine the optimal architecture and hyper-parameter values for the SuperLearnClasssifier for the MNIST Fashion classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the parameter grid to seaerch\n",
    "param_grid = [\n",
    " {'base_estimators': [[\"tree\", \"logreg\"], [\"svm\", \"logreg\", \"knn\", \"tree\"], [\"svm\", \"logreg\", \"knn\", \"tree\", \"randomforest\"]], \n",
    "  'stack_layer_classifier_type':[\"tree\", \"logreg\"],\n",
    "  'estimator_duplicates':[1,2,5,10], \n",
    "  'training_folds': list(range(4, 11, 3)), \n",
    "  'use_probs':[False, True], \n",
    "  'include_base_features_at_stack':[False, True]}  \n",
    "]\n",
    "\n",
    "# Perform the search\n",
    "my_tuned_model = GridSearchCV(SuperLearnerClassifier(), param_grid, cv=cv_folds, verbose = 1, n_jobs=-1)\n",
    "my_tuned_model.fit(X_train_plus_valid, y_train_plus_valid)\n",
    "\n",
    "# Print details\n",
    "print(\"Best parameters set found on development set:\")\n",
    "print(my_tuned_model.best_params_)\n",
    "print(my_tuned_model.best_score_)\n",
    "display(my_tuned_model.grid_scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the perofmrance of the model selected by the grid search on a hold-out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a set of predictions for the test data\n",
    "y_pred = my_tuned_model.predict(X_test)\n",
    "\n",
    "# Print performance details\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"Accuracy: \" +  str(accuracy))\n",
    "print(metrics.classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix\")\n",
    "pd.crosstab(np.array(y_test), y_pred, rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the Impact of Adding Original Descriptive Features at the Stack Layer (Task 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the impact of adding original descriptive features at the stack layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = SuperLearnerClassifier(use_probs=True, include_base_features_at_stack = True, stack_layer_classifier_type = \"logreg\")\n",
    "scores = cross_val_score(my_model, X_train_plus_valid, y_train_plus_valid, cv=cv_folds, n_jobs=-1, verbose = 2)\n",
    "print(\"Mean accuracy \", mean(scores))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Ensemble Model (Task 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an analysis to investigate the strength of the base estimators and the strengths of the correlations between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the strength of the individual classifiers within the ensemble by measureing the accuracy of their predictions on a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Individual accuracies\")\n",
    "\n",
    "# Make a set of predictions for the test data\n",
    "y_pred = my_model.predict(X_test)\n",
    "accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "print(\"** Ensemble  Accuracy: \" +  str(accuracy))\n",
    "    \n",
    "\n",
    "for i in range(my_model.n_estimators_):\n",
    "    \n",
    "    # Make a set of predictions for the training data\n",
    "    y_pred = my_model.last_X_stack_queries[:, i]\n",
    "\n",
    "    # Print performance details\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred) # , normalize=True, sample_weight=None\n",
    "    print(\"** \", i, \" \", my_model.base_estimator_types[i], \" Accuracy: \" +  str(accuracy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measrue the disagreement between base estimators by calculating the Cohen's kappa metric between each of their classicications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kappa_matrix = np.zeros((my_model.n_estimators_, my_model.n_estimators_))\n",
    "for i in range(my_model.n_estimators_):\n",
    "    for j in range(my_model.n_estimators_):\n",
    "        kappa = cohen_kappa_score(my_model.last_X_stack_queries[:, i], my_model.last_X_stack_queries[:, j], labels=None, weights=None)\n",
    "        kappa_matrix[i][j] = kappa\n",
    "        \n",
    "print(kappa_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
